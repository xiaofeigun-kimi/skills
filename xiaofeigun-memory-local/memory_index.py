#!/usr/bin/env python3
"""
å°é£æ£è½»é‡çº§è®°å¿†æœç´¢ç³»ç»Ÿ v2.0
åŸºäº BM25 + å…³é”®è¯ç´¢å¼•ï¼Œæ— éœ€ embedding æ¨¡å‹
æ–°å¢ï¼šè‡ªåŠ¨ç›‘æ§ã€å¢é‡æ›´æ–°ã€æƒé‡ä¼˜åŒ–ã€åŒä¹‰è¯æ”¯æŒ
ä½œè€…ï¼šå°é£æ£ ğŸª„
"""

import os
import json
import re
import hashlib
import time
import threading
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from collections import defaultdict
import math

class MemoryIndex:
    """è½»é‡çº§è®°å¿†ç´¢å¼•å™¨ v2.0"""
    
    # åŒä¹‰è¯è¯å…¸ï¼ˆåŒå‘æ˜ å°„ï¼‰
    SYNONYMS = {
        # ä¸­æ–‡åŒä¹‰è¯
        "å°èå­": ["ç”¨æˆ·", "ä¸»äºº", "æœ‹å‹"],
        "ç”¨æˆ·": ["å°èå­", "ä¸»äºº", "æœ‹å‹"],
        "å°é£æ£": ["æˆ‘", "åŠ©æ‰‹", "AI", "ai"],
        "æˆ‘": ["å°é£æ£", "åŠ©æ‰‹"],
        "è®°å¿†": ["è®°å½•", "æ—¥å¿—", "ç¬”è®°"],
        "è®°å½•": ["è®°å¿†", "æ—¥å¿—"],
        "æ–‡ä»¶": ["æ–‡æ¡£", "èµ„æ–™"],
        "æ–‡æ¡£": ["æ–‡ä»¶", "èµ„æ–™"],
        # è‹±æ–‡åŒä¹‰è¯
        "user": ["human", "person", "friend"],
        "ai": ["assistant", "bot", "agent"],
        "memory": ["record", "log", "note"],
        "file": ["document", "doc"],
    }
    
    # BM25 å‚æ•°
    BM25_K1 = 1.5  # è¯é¢‘é¥±å’Œåº¦
    BM25_B = 0.75  # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–
    
    def __init__(self, workspace_path: str):
        self.workspace = Path(workspace_path)
        self.memory_dir = self.workspace / "memory"
        self.index_dir = self.workspace / ".memory-index"
        self.index_file = self.index_dir / "index.json"
        self.watcher_file = self.index_dir / "watcher.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½ç´¢å¼•
        self.index = self._load_index()
        self.watcher_state = self._load_watcher()
        
        # ç›‘æ§çº¿ç¨‹
        self._watcher_thread = None
        self._watcher_running = False
    
    def _load_index(self) -> Dict:
        """åŠ è½½ç´¢å¼•æ–‡ä»¶"""
        if self.index_file.exists():
            with open(self.index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "version": "2.0",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "files": {},
            "keywords": {},
            "stats": {"total_files": 0, "total_chunks": 0, "total_keywords": 0}
        }
    
    def _load_watcher(self) -> Dict:
        """åŠ è½½ç›‘æ§çŠ¶æ€"""
        if self.watcher_file.exists():
            with open(self.watcher_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"last_check": 0, "file_mtimes": {}}
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        self.index["updated_at"] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)
    
    def _save_watcher(self):
        """ä¿å­˜ç›‘æ§çŠ¶æ€"""
        with open(self.watcher_file, 'w', encoding='utf-8') as f:
            json.dump(self.watcher_state, f, ensure_ascii=False, indent=2)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰"""
        text = text.lower()
        
        # æå–è‹±æ–‡å•è¯
        english_words = re.findall(r'[a-z]+', text)
        
        # æå–ä¸­æ–‡è¯è¯­ï¼ˆç®€å•åˆ†è¯ï¼š2-4å­—è¯ç»„ï¼‰
        chinese_words = []
        for i in range(len(text)):
            for length in [4, 3, 2]:
                if i + length <= len(text):
                    word = text[i:i+length]
                    if '\u4e00' <= word[0] <= '\u9fff':
                        chinese_words.append(word)
        
        # åˆå¹¶å¹¶å»é‡
        all_words = list(set(english_words + chinese_words))
        
        # åœç”¨è¯
        stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
                     'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been',
                     'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 
                     'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 
                     'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä¸ª',
                     'èƒ½', 'å¯ä»¥', 'æŠŠ', 'è®©', 'ç»™', 'è¢«', 'è·Ÿ', 'å¯¹', 'å‘', 'ä»'}
        
        return [w for w in all_words if len(w) > 1 and w not in stopwords]
    
    def _expand_query(self, keywords: List[str]) -> List[str]:
        """æ‰©å±•æŸ¥è¯¢è¯ï¼ˆåŠ å…¥åŒä¹‰è¯ï¼‰"""
        expanded = set(keywords)
        for kw in keywords:
            if kw in self.SYNONYMS:
                expanded.update(self.SYNONYMS[kw])
        return list(expanded)
    
    def _chunk_text(self, text: str) -> List[Dict]:
        """å°†æ–‡æœ¬åˆ†å—ï¼ˆæŒ‰æ ‡é¢˜ï¼‰"""
        chunks = []
        lines = text.split('\n')
        
        current_chunk = []
        current_start = 0
        
        for i, line in enumerate(lines):
            if line.startswith('#') and current_chunk:
                chunk_text = '\n'.join(current_chunk)
                chunks.append({
                    "text": chunk_text,
                    "start_line": current_start + 1,
                    "end_line": i,
                    "hash": hashlib.md5(chunk_text.encode()).hexdigest()[:8]
                })
                current_chunk = [line]
                current_start = i
            else:
                current_chunk.append(line)
        
        if current_chunk:
            chunk_text = '\n'.join(current_chunk)
            chunks.append({
                "text": chunk_text,
                "start_line": current_start + 1,
                "end_line": len(lines),
                "hash": hashlib.md5(chunk_text.encode()).hexdigest()[:8]
            })
        
        return chunks
    
    def _calculate_bm25_score(self, term: str, doc_freq: int, total_docs: int, 
                               term_freq: int, doc_length: int, avg_doc_length: float) -> float:
        """è®¡ç®— BM25 åˆ†æ•°"""
        # IDF è®¡ç®—
        idf = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1)
        
        # è¯é¢‘å½’ä¸€åŒ–
        tf = (term_freq * (self.BM25_K1 + 1)) / \
             (term_freq + self.BM25_K1 * (1 - self.BM25_B + self.BM25_B * (doc_length / avg_doc_length)))
        
        return idf * tf
    
    def _index_single_file(self, file_path: Path) -> bool:
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            chunks = self._chunk_text(content)
            rel_path = str(file_path.relative_to(self.workspace))
            file_stat = file_path.stat()
            
            file_info = {
                "path": rel_path,
                "mtime": file_stat.st_mtime,
                "size": file_stat.st_size,
                "chunks": []
            }
            
            for chunk in chunks:
                keywords = self._extract_keywords(chunk["text"])
                chunk_info = {
                    "hash": chunk["hash"],
                    "start_line": chunk["start_line"],
                    "end_line": chunk["end_line"],
                    "keywords": keywords,
                    "keyword_freq": {kw: keywords.count(kw) for kw in set(keywords)},
                    "length": len(chunk["text"]),
                    "preview": chunk["text"][:200] + "..." if len(chunk["text"]) > 200 else chunk["text"]
                }
                file_info["chunks"].append(chunk_info)
                
                # æ›´æ–°å…³é”®è¯ç´¢å¼•
                for kw in set(keywords):
                    if kw not in self.index["keywords"]:
                        self.index["keywords"][kw] = []
                    self.index["keywords"][kw].append({
                        "file": rel_path,
                        "hash": chunk["hash"],
                        "freq": keywords.count(kw)
                    })
            
            self.index["files"][rel_path] = file_info
            return True
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    def build_index(self, incremental: bool = False):
        """é‡å»ºæˆ–å¢é‡æ›´æ–°ç´¢å¼•"""
        if incremental:
            print("ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°ç´¢å¼•...")
        else:
            print("ğŸ”„ å¼€å§‹é‡å»ºå®Œæ•´ç´¢å¼•...")
            self.index = {
                "version": "2.0",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "files": {},
                "keywords": {},
                "stats": {}
            }
        
        # æ”¶é›†éœ€è¦ç´¢å¼•çš„æ–‡ä»¶
        memory_files = list(self.memory_dir.glob("*.md"))
        memory_md = self.workspace / "MEMORY.md"
        if memory_md.exists():
            memory_files.append(memory_md)
        
        indexed_count = 0
        skipped_count = 0
        
        for file_path in memory_files:
            rel_path = str(file_path.relative_to(self.workspace))
            mtime = file_path.stat().st_mtime
            
            # å¢é‡æ›´æ–°ï¼šè·³è¿‡æœªä¿®æ”¹çš„æ–‡ä»¶
            if incremental and rel_path in self.index["files"]:
                if self.index["files"][rel_path].get("mtime") == mtime:
                    skipped_count += 1
                    continue
            
            print(f"  ğŸ“„ ç´¢å¼•: {file_path.name}")
            if self._index_single_file(file_path):
                indexed_count += 1
        
        # æ¸…ç†å·²åˆ é™¤çš„æ–‡ä»¶
        if incremental:
            current_files = {str(f.relative_to(self.workspace)) for f in memory_files}
            deleted_files = set(self.index["files"].keys()) - current_files
            for deleted in deleted_files:
                print(f"  ğŸ—‘ï¸  ç§»é™¤: {deleted}")
                del self.index["files"][deleted]
        
        # æ›´æ–°ç»Ÿè®¡
        total_chunks = sum(len(f["chunks"]) for f in self.index["files"].values())
        self.index["stats"] = {
            "total_files": len(self.index["files"]),
            "total_chunks": total_chunks,
            "total_keywords": len(self.index["keywords"])
        }
        
        self._save_index()
        
        if incremental:
            print(f"âœ… å¢é‡æ›´æ–°å®Œæˆï¼æ–°å¢/æ›´æ–°: {indexed_count}, è·³è¿‡: {skipped_count}, åˆ é™¤: {len(deleted_files) if incremental else 0}")
        else:
            print(f"âœ… é‡å»ºå®Œæˆï¼æ–‡ä»¶: {self.index['stats']['total_files']}, å—: {total_chunks}, å…³é”®è¯: {len(self.index['keywords'])}")
    
    def _is_hot_memory(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºçƒ­è®°å¿†ï¼ˆä»Šå¤©æˆ–æ˜¨å¤©çš„æ–‡ä»¶ï¼‰"""
        from datetime import datetime, timedelta
        
        today = datetime.now().strftime("%Y-%m-%d")
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«ä»Šå¤©æˆ–æ˜¨å¤©çš„æ—¥æœŸ
        if today in file_path or yesterday in file_path:
            return True
        
        # MEMORY.md å§‹ç»ˆæ˜¯çƒ­è®°å¿†
        if "MEMORY.md" in file_path:
            return True
        
        return False
    
    def _search_in_hot_memory(self, query_keywords: List[str], top_k: int) -> List[Dict]:
        """åœ¨çƒ­è®°å¿†ä¸­æœç´¢ï¼ˆä»Šå¤©+æ˜¨å¤©çš„æ–‡ä»¶ï¼‰"""
        hot_scores = defaultdict(lambda: {"score": 0, "matched_keywords": [], "term_freqs": {}})
        
        # åªéå†çƒ­è®°å¿†æ–‡ä»¶
        for file_path, file_info in self.index["files"].items():
            if not self._is_hot_memory(file_path):
                continue
            
            for chunk in file_info["chunks"]:
                chunk_text = chunk.get("text", "").lower()
                
                for kw in query_keywords:
                    if kw in chunk_text:
                        # ç®€åŒ–è¯„åˆ†ï¼šå…³é”®è¯åŒ¹é…æ¬¡æ•°
                        freq = chunk_text.count(kw)
                        if freq > 0:
                            key = (file_path, chunk["hash"])
                            hot_scores[key]["score"] += freq * 2.0  # çƒ­è®°å¿†åŠ æƒ
                            hot_scores[key]["matched_keywords"].append(kw)
        
        if not hot_scores:
            return []
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_scores = sorted(hot_scores.items(), key=lambda x: x[1]["score"], reverse=True)
        
        # ç»„è£…ç»“æœ
        results = []
        for (file_path, chunk_hash), data in sorted_scores[:top_k]:
            file_info = self.index["files"].get(file_path)
            if file_info:
                for chunk in file_info["chunks"]:
                    if chunk["hash"] == chunk_hash:
                        results.append({
                            "path": file_path,
                            "start_line": chunk["start_line"],
                            "end_line": chunk["end_line"],
                            "preview": chunk["preview"],
                            "score": round(data["score"], 4),
                            "matched_keywords": list(set(data["matched_keywords"])),
                            "is_hot": True
                        })
                        break
        
        return results
    
    def search(self, query: str, top_k: int = 5, use_hot_memory_first: bool = True) -> List[Dict]:
        """BM25 æœç´¢è®°å¿†ï¼ˆæ”¯æŒçƒ­è®°å¿†å¿«é€Ÿé€šé“ï¼‰"""
        if not self.index["files"]:
            return []
        
        # æå–å¹¶æ‰©å±•æŸ¥è¯¢è¯
        query_keywords = self._extract_keywords(query)
        query_keywords = self._expand_query(query_keywords)
        
        if not query_keywords:
            return []
        
        # çƒ­è®°å¿†å¿«é€Ÿé€šé“ï¼šå…ˆæœä»Šå¤©+æ˜¨å¤©çš„è®°å¿†
        if use_hot_memory_first:
            hot_results = self._search_in_hot_memory(query_keywords, top_k)
            if hot_results and hot_results[0]["score"] > 1.0:  # å¦‚æœçƒ­è®°å¿†æœ‰é«˜è´¨é‡åŒ¹é…
                return hot_results[:top_k]
        
        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        total_length = sum(
            sum(c["length"] for c in f["chunks"]) 
            for f in self.index["files"].values()
        )
        total_chunks = self.index["stats"].get("total_chunks", 1)
        avg_doc_length = total_length / total_chunks if total_chunks > 0 else 1
        
        total_docs = total_chunks
        
        # è®¡ç®—æ¯ä¸ªå—çš„ BM25 åˆ†æ•°
        scores = defaultdict(lambda: {"score": 0, "matched_keywords": [], "term_freqs": {}})
        
        for kw in query_keywords:
            if kw in self.index["keywords"]:
                doc_freq = len(self.index["keywords"][kw])
                
                for match in self.index["keywords"][kw]:
                    key = (match["file"], match["hash"])
                    
                    # è·å–å—ä¿¡æ¯
                    file_info = self.index["files"].get(match["file"])
                    if not file_info:
                        continue
                    
                    chunk_info = None
                    for c in file_info["chunks"]:
                        if c["hash"] == match["hash"]:
                            chunk_info = c
                            break
                    
                    if chunk_info:
                        term_freq = match["freq"]
                        doc_length = chunk_info["length"]
                        
                        # è®¡ç®— BM25 åˆ†æ•°
                        score = self._calculate_bm25_score(
                            kw, doc_freq, total_docs, term_freq, doc_length, avg_doc_length
                        )
                        
                        scores[key]["score"] += score
                        scores[key]["matched_keywords"].append(kw)
                        scores[key]["term_freqs"][kw] = term_freq
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_scores = sorted(scores.items(), key=lambda x: x[1]["score"], reverse=True)
        
        # ç»„è£…ç»“æœ
        results = []
        for (file_path, chunk_hash), data in sorted_scores[:top_k]:
            file_info = self.index["files"].get(file_path)
            if file_info:
                for chunk in file_info["chunks"]:
                    if chunk["hash"] == chunk_hash:
                        results.append({
                            "path": file_path,
                            "start_line": chunk["start_line"],
                            "end_line": chunk["end_line"],
                            "preview": chunk["preview"],
                            "score": round(data["score"], 4),
                            "matched_keywords": list(set(data["matched_keywords"]))
                        })
                        break
        
        return results
    
    def check_for_changes(self) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–"""
        memory_files = list(self.memory_dir.glob("*.md"))
        memory_md = self.workspace / "MEMORY.md"
        if memory_md.exists():
            memory_files.append(memory_md)
        
        current_mtimes = {}
        for file_path in memory_files:
            rel_path = str(file_path.relative_to(self.workspace))
            current_mtimes[rel_path] = file_path.stat().st_mtime
        
        # å¯¹æ¯”æ—§çŠ¶æ€
        old_mtimes = self.watcher_state.get("file_mtimes", {})
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
        has_changes = False
        
        # æ–°æ–‡ä»¶æˆ–ä¿®æ”¹çš„æ–‡ä»¶
        for path, mtime in current_mtimes.items():
            if path not in old_mtimes or old_mtimes[path] != mtime:
                has_changes = True
                break
        
        # åˆ é™¤çš„æ–‡ä»¶
        if not has_changes:
            for path in old_mtimes:
                if path not in current_mtimes:
                    has_changes = True
                    break
        
        # æ›´æ–°ç›‘æ§çŠ¶æ€
        self.watcher_state["file_mtimes"] = current_mtimes
        self.watcher_state["last_check"] = time.time()
        self._save_watcher()
        
        return has_changes
    
    def start_watcher(self, interval: int = 30):
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§çº¿ç¨‹"""
        if self._watcher_running:
            print("âš ï¸ ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        self._watcher_running = True
        
        def watch_loop():
            while self._watcher_running:
                try:
                    if self.check_for_changes():
                        print(f"\nğŸ“ æ£€æµ‹åˆ°è®°å¿†æ–‡ä»¶å˜åŒ–ï¼Œè‡ªåŠ¨æ›´æ–°ç´¢å¼•...")
                        self.build_index(incremental=True)
                        print("ğŸ’¡ å¯ä»¥ç»§ç»­è¾“å…¥å‘½ä»¤\n> ", end="", flush=True)
                except Exception as e:
                    print(f"âŒ ç›‘æ§é”™è¯¯: {e}")
                
                time.sleep(interval)
        
        self._watcher_thread = threading.Thread(target=watch_loop, daemon=True)
        self._watcher_thread.start()
        print(f"ğŸ‘ï¸ æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨ï¼ˆæ¯ {interval} ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰")
    
    def stop_watcher(self):
        """åœæ­¢æ–‡ä»¶ç›‘æ§"""
        self._watcher_running = False
        if self._watcher_thread:
            self._watcher_thread.join(timeout=1)
        print("ğŸ›‘ æ–‡ä»¶ç›‘æ§å·²åœæ­¢")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    workspace = os.environ.get("OPENCLAW_WORKSPACE", "/root/.openclaw/workspace")
    indexer = MemoryIndex(workspace)
    
    if len(sys.argv) < 2:
        print("ğŸª„ å°é£æ£è®°å¿†æœç´¢ç³»ç»Ÿ v2.0")
        print("\nç”¨æ³•: python memory_index.py <command> [args]")
        print("\nå‘½ä»¤:")
        print("  build              - é‡å»ºå®Œæ•´ç´¢å¼•")
        print("  update             - å¢é‡æ›´æ–°ç´¢å¼•")
        print("  search <query>     - æœç´¢è®°å¿†")
        print("  watch              - å¯åŠ¨æ–‡ä»¶ç›‘æ§")
        print("  stop               - åœæ­¢æ–‡ä»¶ç›‘æ§")
        print("  stats              - æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡")
        print("  check              - æ£€æŸ¥æ–‡ä»¶å˜åŒ–")
        return
    
    command = sys.argv[1]
    
    if command == "build":
        indexer.build_index(incremental=False)
    
    elif command == "update":
        indexer.build_index(incremental=True)
    
    elif command == "search":
        query = " ".join(sys.argv[2:])
        results = indexer.search(query)
        print(f"\nğŸ” æœç´¢: '{query}'")
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
        for i, r in enumerate(results, 1):
            print(f"{i}. {r['path']} (è¡Œ {r['start_line']}-{r['end_line']}, åˆ†æ•°: {r['score']})")
            print(f"   åŒ¹é…: {', '.join(r['matched_keywords'])}")
            print(f"   {r['preview'][:150]}...")
            print()
    
    elif command == "watch":
        indexer.start_watcher()
        print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§...")
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            indexer.stop_watcher()
    
    elif command == "stop":
        indexer.stop_watcher()
    
    elif command == "stats":
        print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡:")
        print(f"  ç‰ˆæœ¬: {indexer.index['version']}")
        print(f"  åˆ›å»ºæ—¶é—´: {indexer.index['created_at']}")
        print(f"  æ›´æ–°æ—¶é—´: {indexer.index['updated_at']}")
        print(f"  æ–‡ä»¶æ•°: {indexer.index['stats'].get('total_files', 0)}")
        print(f"  å—æ•°: {indexer.index['stats'].get('total_chunks', 0)}")
        print(f"  å…³é”®è¯æ•°: {indexer.index['stats'].get('total_keywords', 0)}")
    
    elif command == "check":
        if indexer.check_for_changes():
            print("ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–")
        else:
            print("âœ… æ–‡ä»¶æ— å˜åŒ–")
    
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")


if __name__ == "__main__":
    main()
